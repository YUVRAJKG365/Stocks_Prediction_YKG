# -*- coding: utf-8 -*-
"""ptr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v2U95jxnKp4k4olRg-QQPGrF9WHzy2av
"""

from google.colab import files
import io

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# To read the file into a pandas DataFrame:
# import pandas as pd
# df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')))

import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
from sklearn.utils import resample

# Load and preprocess dataset
def load_and_preprocess(file_path):
    df = pd.read_csv(file_path)

    df['Date'] = pd.date_range(start='2022-01-01', periods=len(df), freq='D')
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Month'] = df['Date'].dt.month
    df['Quarter'] = df['Date'].dt.quarter
    df['Year'] = df['Date'].dt.year

    # FIX 1: Avoid chained assignment warning
    df['DiscountedPrice'] = df['DiscountedPrice'].fillna(df['DiscountedPrice'].median())
    df['Price'] = df['Price'].fillna(df['Price'].median())

    df['DiscountPct'] = ((df['Price'] - df['DiscountedPrice']) / df['Price']) * 100

    le = LabelEncoder()
    df['CategoryEncoded'] = le.fit_transform(df['Category'])
    df['BrandEncoded'] = le.fit_transform(df['Brand'])

    holidays = ['2022-01-01', '2022-01-26', '2022-08-15', '2022-10-02',
                '2023-01-01', '2023-01-26', '2023-08-15', '2023-10-02']
    df['IsHoliday'] = df['Date'].isin(pd.to_datetime(holidays)).astype(int)

    np.random.seed(42)
    base_sales = np.random.randint(50, 200, size=len(df))
    month_effect = df['Month'].apply(lambda x: 1.2 if x in [11, 12] else 1.0)
    discount_effect = 1 + (df['DiscountPct'] / 100) * 0.5
    holiday_boost = df['IsHoliday'].apply(lambda x: 1.3 if x == 1 else 1.0)

    df['UnitsSold'] = base_sales * month_effect * discount_effect * holiday_boost
    df['UnitsSold'] += np.random.normal(0, 10, len(df))

    # FIX 2: Handle potential NaNs before casting to int
    df['UnitsSold'] = df['UnitsSold'].fillna(0)
    df['UnitsSold'] = df['UnitsSold'].astype(int)

    features = ['DayOfWeek', 'Month', 'Quarter', 'Year', 'DiscountPct',
                'IsHoliday', 'CategoryEncoded', 'BrandEncoded']

    return df[features + ['UnitsSold', 'Date']]


# Balance dataset
def balance_dataset(df):
    df['SalesCategory'] = pd.qcut(df['UnitsSold'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])

    low = df[df['SalesCategory'] == 'Low']
    medium = df[df['SalesCategory'] == 'Medium']
    high = df[df['SalesCategory'] == 'High']
    very_high = df[df['SalesCategory'] == 'Very High']

    medium_upsampled = resample(medium, replace=True, n_samples=len(low), random_state=42)
    high_upsampled = resample(high, replace=True, n_samples=len(low), random_state=42)
    very_high_upsampled = resample(very_high, replace=True, n_samples=len(low), random_state=42)

    balanced_df = pd.concat([low, medium_upsampled, high_upsampled, very_high_upsampled])

    return balanced_df.drop('SalesCategory', axis=1)

# Main workflow
def main():
    df = load_and_preprocess('DMart.csv')
    balanced_df = balance_dataset(df)

    balanced_df = balanced_df.sort_values('Date')
    split_index = int(len(balanced_df) * 0.8)
    train = balanced_df.iloc[:split_index]
    test = balanced_df.iloc[split_index:]

    X_train = train.drop(['UnitsSold', 'Date'], axis=1)
    y_train = train['UnitsSold']
    X_test = test.drop(['UnitsSold', 'Date'], axis=1)
    y_test = test['UnitsSold']

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=7,
        subsample=0.8,
        colsample_bytree=0.8,
        early_stopping_rounds=50,
        random_state=42
    )

    tscv = TimeSeriesSplit(n_splits=5)

    model.fit(X_train_scaled, y_train,
              eval_set=[(X_test_scaled, y_test)],
              verbose=False)

    y_pred = model.predict(X_test_scaled)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f"Model Evaluation Metrics:")
    print(f"MAE: {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"RÂ²: {r2:.4f}")

    plt.figure(figsize=(10, 6))
    xgb.plot_importance(model)
    plt.title('Feature Importance')
    plt.show()

    last_date = balanced_df['Date'].max()
    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)

    forecast_df = balanced_df.tail(30).copy()
    forecast_df['Date'] = forecast_dates
    forecast_df['DayOfWeek'] = forecast_dates.dayofweek
    forecast_df['Month'] = forecast_dates.month
    forecast_df['Quarter'] = (forecast_dates.month - 1) // 3 + 1
    forecast_df['Year'] = forecast_dates.year

    forecast_holidays = ['2023-12-25', '2024-01-01', '2024-01-26']
    forecast_df['IsHoliday'] = forecast_df['Date'].isin(pd.to_datetime(forecast_holidays)).astype(int)

    X_forecast = forecast_df.drop(['UnitsSold', 'Date'], axis=1)
    X_forecast_scaled = scaler.transform(X_forecast)

    predictions = model.predict(X_forecast_scaled)

    residuals = y_test - model.predict(X_test_scaled)
    std_dev = residuals.std()
    conf_interval = 1.96 * std_dev

    forecast_results = pd.DataFrame({
        'Date': forecast_dates,
        'PredictedUnits': predictions,
        'LowerBound': predictions - conf_interval,
        'UpperBound': predictions + conf_interval
    })

    plt.figure(figsize=(14, 7))
    plt.plot(balanced_df['Date'], balanced_df['UnitsSold'], 'b-', label='Historical Sales')
    plt.plot(forecast_results['Date'], forecast_results['PredictedUnits'], 'r-', label='Forecast')
    plt.fill_between(forecast_results['Date'],
                     forecast_results['LowerBound'],
                     forecast_results['UpperBound'],
                     color='pink', alpha=0.3, label='95% Confidence Interval')

    plt.title('30-Day Product Demand Forecast')
    plt.xlabel('Date')
    plt.ylabel('Units Sold')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    print("\n30-Day Demand Forecast:")
    print(forecast_results[['Date', 'PredictedUnits']].to_string(index=False))

    print("\nEthics and Fairness Considerations:")
    print("1. Dataset represents diverse product categories and brands")
    print("2. No sensitive demographic features included")
    print("3. Balanced sampling technique used to prevent minority class underrepresentation")
    print("4. Regular monitoring for performance across product categories recommended")

if __name__ == "__main__":
    main()

# Assuming your original preprocessing functions exist in the notebook:

# 1. Load original data file (replace with the correct filename if needed)
df = load_and_preprocess('DMart.csv')

# 2. Balance the dataset using your defined function
balanced_df = balance_dataset(df)

def enrich_features(df):
    df = df.sort_values("Date").reset_index(drop=True)

    # Lag features
    df['Lag_1'] = df['UnitsSold'].shift(1)
    df['Lag_7'] = df['UnitsSold'].shift(7)

    # Date/time-based features
    df['DayOfYear'] = df['Date'].dt.dayofyear
    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)
    df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)

    # Rolling average (7-day)
    df['RollingMean_7'] = df['UnitsSold'].rolling(window=7).mean()

    # âœ… FIX: Use .bfill() instead of deprecated fillna
    df = df.bfill()

    return df

# Apply enrichment to entire dataset
df_enriched = enrich_features(balanced_df)

# Redefine features
features = [
    'DayOfWeek', 'Month', 'Quarter', 'Year', 'DiscountPct',
    'IsHoliday', 'CategoryEncoded', 'BrandEncoded',
    'Lag_1', 'Lag_7', 'DayOfYear', 'WeekOfYear', 'IsWeekend', 'RollingMean_7'
]

from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

# Split
split_index = int(len(df_enriched) * 0.8)
train = df_enriched.iloc[:split_index]
test = df_enriched.iloc[split_index:]

X_train = train[features]
y_train = train['UnitsSold']
X_test = test[features]
y_test = test['UnitsSold']

# Scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.05, max_depth=7, random_state=42),
    'LightGBM': LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=7, random_state=42),
    'CatBoost': CatBoostRegressor(verbose=0, n_estimators=1000, learning_rate=0.05, max_depth=7, random_state=42),
    'RandomForest': RandomForestRegressor(n_estimators=300, max_depth=15, random_state=42)
}

results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    results[name] = {
        'Model': model,
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'Predictions': y_pred
    }

# Display all results
print("\nðŸ“Š Model Performance Comparison:")
for name, metrics in results.items():
    print(f"\n{name} Model:")
    print(f"MAE : {metrics['MAE']:.2f}")
    print(f"RMSE: {metrics['RMSE']:.2f}")
    print(f"RÂ²  : {metrics['R2']:.4f}")

# Select best model based on RÂ²
best_model_name = max(results, key=lambda k: results[k]['R2'])
best_model = results[best_model_name]['Model']
best_preds = results[best_model_name]['Predictions']

print(f"\nâœ… Best Model: {best_model_name}")

# Visualize
plt.figure(figsize=(12, 5))
plt.plot(y_test.values, label='Actual', color='blue', alpha=0.6)
plt.plot(best_preds, label=f'Predicted by {best_model_name}', color='green', alpha=0.7)
plt.title(f"{best_model_name}: Actual vs Predicted Units Sold")
plt.xlabel("Test Samples")
plt.ylabel("Units Sold")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib
joblib.dump(best_model, 'best_random_forest_model.pkl')

from google.colab import files

files.download('best_random_forest_model.pkl')